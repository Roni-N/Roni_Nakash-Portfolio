---
layout: default
---


# Hello and welcome to my portfolio!
------------------------------------------------------------

* * *

***

*****

- - -

## My name is Roni Nakash, and I'm a Data Scientist.

I have a Master's (MS.c) in Industrial Engineering (specialization in Data Science) 
and a bachelor's (BS.c) in Industrial Engineering (specialization in information systems) 
from Ben Gurion University.


In this portfolio, I will share with you some of my knowledge in the fields of
Data Science, Machine Learning, Artificial Intelligence Statistics, Python and more.

I hope you will find this information helpful, and I would love to hear your opinion :)
Please feel free to contact me in any matter!
Cheers :)




------------------------------------------------------------
# Classification Project: Insurance claim prediction 

Insurance claim prediction project.
This project is a supervised learning classification task based on structured data.
In this project we will develop an Insurance claim prediction model using different data sources.

In this project, we will use the CRISP-DM methodology and popular data science and machine learning techniques, such as:  
data integration, exploratory data analysis (EDA), feature engineering, feature selection, imbalance date methods, machine learning models, evaluation and  hyperparameters tuning using: python libraries (Numpy, Pandas, Matplotlib, Scikit-learn), statistics, and more.




[![](https://img.shields.io/badge/GitHub-Full%20project%20Link-blue?logo=Github)](https://roni-n.github.io/Insurance-claim-prediction/)


[![]("assets/img/gg2.png")](https://roni-n.github.io/Insurance-claims-prediction/)



[project 1](https://roni-n.github.io/Insurance-claims-prediction/).


[project 2](https://roni-n.github.io/Startup-Success-Prediction/).


# Insurance claims prediction project.
This project is a supervised learning classification task based on structured data.
In this project we will develop an Insurance claim prediction model using different data sources.

An insurance claim prediction model could be a valuable and powerful tool. 
By using this tool, insurance companies can minimize risks, provide adaptive pricing premiums, streamline work processes, reduce costs and maximize profits.

In this project, we will use the CRISP-DM methodology. 
CRISP-DM is a popular workflow, including six steps (fig 1). In each step, we will perform different techniques that will help us to build and develop an efficient model.

------------------------------------------------------------

## Section 1: Business Understanding
In this section, we will learn about the business world (the insurance industry), and we will define the business problem and our project objectives.
Pic 1 and link


------------------------------------------------------------
## Section 2: Data Understanding
************************************************************

### 2.1 Data integration 
In this section, firstly, we will explore the available data (features) from the different sources. 
We will integrate the data into a unified dataset, and we will produce the target variable.
Pic 1 and link
************************************************************

### 2.2 Exploratory Data Analysis (EDA)
In the next step, we will perform Exploratory Data Analysis (EDA) to perform initial investigations, discover patterns, spot anomalies, test hypotheses, and check assumptions with the help of summary statistics and graphical representations.
Pic 1 and link

------------------------------------------------------------
## Section 3: Data preparation 
### 3.1 Feature Engineering:
In this section, we will use different methods and techniques to transform and extract raw data into features that better represent the underlying problem to the predictive model.
The main steps include:
Missing Data Imputation
Categorical Features Encoding
Transformations
Discretisations
Outliers Handling
Features Scaling
Engineering mixed Features
Pic 1 and link
************************************************************
### 3.2 Feature Selection: 
In the next step, we will perform Feature selection methods to reduce (removing non-informative or redundant) the number of input variables to those that we believe to be most useful to a model in order to predict the target variable. 
We have three main approaches to handle this step:
Filter methods (variance and statistical test, correlations, univariate selection)
Wrapper methods (forward / backward selection, exhaustive search)
Embedded methods (lasso, tree importance)
Pic 1 and link
************************************************************
### 3.3 Imbalance Data: 
The final step in this section will be Imbalanced data handling. In our data set, the number of observations per class is not equally distributed. This issue cloud encountered a significant drawback of the performance attainable by most standard classifier learning algorithms.
We have three main approaches to handle this step:
Undersampling (Fixed and Cleaning methods)
Oversampling ( sample extraction and sample generation methods)
Ensemble Method (data level, cost-sensitive, ensemble algorithms)
Pic 1 and link
************************************************************

------------------------------------------------------------
## Section 4: Machine Learning Algorithms 
In this section, we will train different machine learning classification algorithms.
We will train models like:
Logistic regression 
SVM
Naive Bayes classifier  
Random Forest
KNN classifier 
And more

------------------------------------------------------------
## Section 5: Evaluation and Hyperparameters Tuning 
In this section, we will evaluate the model's performance by performance metrics.
And we will perform Hyperparameters Tuning methods (i.e. choosing a set of optimal hyperparameters for a learning algorithm) to improve model performance.
In this step, we will use popular methods as:
Cross-validation
Grid search
And more

------------------------------------------------------------



------------------------------------------------------------
# Natural Language Processing (NLP) & Text classification


In this project, we will use the CRISP-DM methodology and popular data science and machine learning techniques, such as:  
data integration, exploratory data analysis (EDA), feature engineering, feature selection, imbalance date methods, machine learning models, evaluation and  hyperparameters tuning using: python libraries (Numpy, Pandas, Matplotlib, Scikit-learn), statistics, and more.


[![](https://img.shields.io/badge/GitHub-Full%20project%20Link-blue?logo=Github)](https://roni-n.github.io/Natural-Language-Processing-NLP-Text-classification/)








------------------------------------------------------------

# Header 1

This is a normal paragraph following a header. GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere.

## Header 2

> This is a blockquote following a header.
>
> When something is important enough, you do it even if the odds are not in your favor.

### Header 3

```js
// Javascript code with syntax highlighting.
var fun = function lang(l) {
  dateformat.i18n = require('./lang/' + l)
  return true;
}
```

```ruby
# Ruby code with syntax highlighting
GitHubPages::Dependencies.gems.each do |gem, version|
  s.add_dependency(gem, "= #{version}")
end
```

#### Header 4

*   This is an unordered list following a header.
*   This is an unordered list following a header.
*   This is an unordered list following a header.

##### Header 5

1.  This is an ordered list following a header.
2.  This is an ordered list following a header.
3.  This is an ordered list following a header.

###### Header 6

| head1        | head two          | three |
|:-------------|:------------------|:------|
| ok           | good swedish fish | nice  |
| out of stock | good and plenty   | nice  |
| ok           | good `oreos`      | hmm   |
| ok           | good `zoute` drop | yumm  |

### There's a horizontal rule below this.

* * *

### Here is an unordered list:

*   Item foo
*   Item bar
*   Item baz
*   Item zip

### And an ordered list:

1.  Item one
1.  Item two
1.  Item three
1.  Item four

### And a nested list:

- level 1 item
  - level 2 item
  - level 2 item
    - level 3 item
    - level 3 item
- level 1 item
  - level 2 item
  - level 2 item
  - level 2 item
- level 1 item
  - level 2 item
  - level 2 item
- level 1 item

### Small image

![Octocat](https://github.githubassets.com/images/icons/emoji/octocat.png)

### Large image

![Branching](https://guides.github.com/activities/hello-world/branching.png)


### Definition lists can be used with HTML syntax.

<dl>
<dt>Name</dt>
<dd>Godzilla</dd>
<dt>Born</dt>
<dd>1952</dd>
<dt>Birthplace</dt>
<dd>Japan</dd>
<dt>Color</dt>
<dd>Green</dd>
</dl>

```
Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.
```

```
The final element.
```
