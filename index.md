---
layout: default
---


# Hello and welcome to my portfolio!
------------------------------------------------------------

>In this portfolio, I will share with you some of my expertise in: <br>
>**Data Science & Machine Learning techniques, Natural Language Processing (NLP) & Text classification, Statistics and more.** <br>

>I hope you will find this information useful, and I would appreciate hearing your thoughts! <br>
>Please don't hesitate to contact me if you have any questions. <br>
>**Thanks and enjoy your reading. :)**

------------------------------------------------------------
## Classification Project: Insurance claim prediction 
------------------------------------------------------------
> **Project purpose:** The purpose of this project is to conduct a supervised learning classification task based on structured data.  <br>

> **Project objective:** The objective of this project is to develop a claim prediction model from different data sets using data science and machine learning  methods (both theoretical and practical).<br> The purpose of this model is to predict (and determine the probability) whether future policyholders will make a future claim based on their characteristics.

> **Motivation:** A claim prediction model could be a valuable and powerful tool for the insurance industry. With this tool, insurance companies can minimize risks, provide customized > pricing and premiums, streamline operational processes, drive down costs, and increase profit margins.


**The project follows the CRISP-DM methodology, which includes the following steps:** <br>
![](/assets/img/wf1.png)
                                                      
As part of my work, I will perform data science and machine learning techniques, including:
1. Integrated Data sets. 
2. Exploratory Data Analysis (EDA). 
3. Feature Engineering. 
4. Feature Selection.
5. Imbalance Date methods. 
6. Machine Learning models. 
7. Evaluation and hyperparameter tuning. <br>
Using **Numpy, Pandas, Matplotlib, Scikit-Learn, and other Python libraries.**

------------------------------------------------------------

### Section 1: Business Understanding
In this section, we will learn about the business world (the insurance industry) and define the business problem and the objectives of our project.<br>

>**Topics covered:**
>- Introduction to the insurance business model.
>- Terms such as "The Law of Large Numbers", "Expected Value", and "Expected Return".
>- Definition of the business problem and the project objectives.


[![](https://img.shields.io/badge/GitHub-Business%20Understanding%20explanation-blue?logo=Github)](https://github.com/Roni-N/Insurance-claim-prediction/blob/gh-pages/Section%201%20Business%20Understanding/(ICP)%200.%20Business%20Understanding..ipynb)

------------------------------------------------------------
### Section 2: Data Understanding

#### 2.1 Data integration 
In this section, we will first explore the available data & features from different datasets. 
Next, we will integrate the data into a unified dataset by implementing the business workflow, and finally, we will produce the target variable.
**Topics covered:**
- Integrate datasets through business workflow
- Comparing Pandas library to SQL basic functions 
- Produce ground truth (label of target variable) 
Pic 1 and link


[![](https://img.shields.io/badge/GitHub-2.1%20Data%20integration%20code-blue?logo=Github)](https://github.com/Roni-N/Insurance-claim-prediction/blob/gh-pages/Section%202%20Data%20Understanding/2.1%20Data%20integration/(ICP)%201.%20Data%20Grouping%20and%20Aggregation..ipynb)

************************************************************

#### 2.2 Exploratory Data Analysis (EDA)
In the next step, we will perform Exploratory Data Analysis (EDA) to perform preliminary investigations, discover patterns, identify anomalies, test hypotheses, and correct assumptions by examining summary statistics and graphs.
**Topics covered:**

------------------------------------------------------------
### Section 3: Data preparation 
#### 3.1 Feature Engineering:
In this section, we will use multiple methods and techniques to convert (process & transform) raw data into features that better represent the underlying problem in a predictive model.
**Topics covered:**
- Missing Data Imputation
- Categorical Features Encoding
- Transformations
- Discretisations
- Outliers Handling
- Features Scaling
- Engineering New Features
Pic 1 and link

************************************************************
#### 3.2 Feature Selection: 
In the next step, we will use feature selection methods to reduce the number of input variables (i.e. remove non-informative or redundant features) to those that will provide the greatest level of predictive power for the target variable.
**Topics covered:**
- Filter methods (variance and statistical test, correlations, univariate selection)
- Wrapper methods (forward / backward selection, exhaustive search)
- Embedded methods (lasso, tree importance)
Pic 1 and link

************************************************************
#### 3.3 Imbalance Data: 
This section will conclude with Imbalanced data handling. Our data set does not have an equally distributed distribution of observations by class. Most standard classifier learning algorithms performed poorly in this issue cloud. It can be resolved by balancing the two classes.
**Topics covered:**
- Undersampling (Fixed and Cleaning methods)
- Oversampling ( sample extraction and sample generation methods)
- Ensemble Method (data level, cost-sensitive, ensemble algorithms)
Pic 1 and link

************************************************************

------------------------------------------------------------
### Section 4: Machine Learning Algorithms 
In this section, we will train different machine learning algorithms to create models that could predict future claim insurance premiums.
**Classification models such as:**
- Logistic regression 
- SVM
- Naive Bayes classifier  
- Random Forest
- KNN classifier 

------------------------------------------------------------
### Section 5: Evaluation and Hyperparameters Tuning 
In this section, we will evaluate the model's performance based on relevant performance metrics.
We will also use Hyperparameter Tuning methods (i.e. choosing a set of optimal hyperparameters) to improve model performance.
**Topics covered:**
- Cross-validation
- Search Algorithms (Grid search, Random search)
- Bayesian Optimization

------------------------------------------------------------



------------------------------------------------------------
## Natural Language Processing (NLP) & Text classification


In this project, we will use the CRISP-DM methodology and popular data science and machine learning techniques, such as:  
data integration, exploratory data analysis (EDA), feature engineering, feature selection, imbalance date methods, machine learning models, evaluation and  hyperparameters tuning using: python libraries (Numpy, Pandas, Matplotlib, Scikit-learn), statistics, and more.


[![](https://img.shields.io/badge/GitHub-Full%20project%20Link-blue?logo=Github)](https://roni-n.github.io/Natural-Language-Processing-NLP-Text-classification/)







[![](https://img.shields.io/badge/GitHub-Full%20project%20Link-blue?logo=Github)](https://roni-n.github.io/Insurance-claim-prediction/)

------------------------------------------------------------

# Header 1

This is a normal paragraph following a header. GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere.

## Header 2

> This is a blockquote following a header.
>
> When something is important enough, you do it even if the odds are not in your favor.

### Header 3

```js
// Javascript code with syntax highlighting.
var fun = function lang(l) {
  dateformat.i18n = require('./lang/' + l)
  return true;
}
```

```ruby
# Ruby code with syntax highlighting
GitHubPages::Dependencies.gems.each do |gem, version|
  s.add_dependency(gem, "= #{version}")
end
```

#### Header 4

*   This is an unordered list following a header.
*   This is an unordered list following a header.
*   This is an unordered list following a header.

##### Header 5

1.  This is an ordered list following a header.
2.  This is an ordered list following a header.
3.  This is an ordered list following a header.

###### Header 6

| head1        | head two          | three |
|:-------------|:------------------|:------|
| ok           | good swedish fish | nice  |
| out of stock | good and plenty   | nice  |
| ok           | good `oreos`      | hmm   |
| ok           | good `zoute` drop | yumm  |

### There's a horizontal rule below this.

* * *

### Here is an unordered list:

*   Item foo
*   Item bar
*   Item baz
*   Item zip

### And an ordered list:

1.  Item one
1.  Item two
1.  Item three
1.  Item four

### And a nested list:

- level 1 item
  - level 2 item
  - level 2 item
    - level 3 item
    - level 3 item
- level 1 item
  - level 2 item
  - level 2 item
  - level 2 item
- level 1 item
  - level 2 item
  - level 2 item
- level 1 item

### Small image

![Octocat](https://github.githubassets.com/images/icons/emoji/octocat.png)

### Large image

![Branching](https://guides.github.com/activities/hello-world/branching.png)


### Definition lists can be used with HTML syntax.

<dl>
<dt>Name</dt>
<dd>Godzilla</dd>
<dt>Born</dt>
<dd>1952</dd>
<dt>Birthplace</dt>
<dd>Japan</dd>
<dt>Color</dt>
<dd>Green</dd>
</dl>

```
Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.
```

```
The final element.
```
