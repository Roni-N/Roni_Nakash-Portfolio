---
layout: default
---


# Hello and welcome to my portfolio!
------------------------------------------------------------

>In this portfolio, I will share with you some of my expertise in: <br>
>**Data Science & Machine Learning techniques, Natural Language Processing (NLP) & Text classification, Statistics and more.** <br>

>I hope you will find this information useful, and I would appreciate hearing your thoughts! <br>
>Please don't hesitate to contact me if you have any questions. <br>
>**Thanks and enjoy your reading. :)**

------------------------------------------------------------
## <span style="color:blue">Classification Project:<br> Insurance claim prediction</span>.
------------------------------------------------------------
> **Project purpose:** The purpose of this project is to conduct a supervised learning classification task based on structured data.  <br>

> **Project objective:** The objective of this project is to develop a claim prediction model from different data sets using data science and machine learning  methods (both theoretical and practical).<br> The purpose of this model is to predict (and determine the probability) whether future policyholders will make a future claim based on their characteristics.

> **Motivation:** A claim prediction model could be a valuable and powerful tool for the insurance industry. With this tool, insurance companies can minimize risks, provide customized pricing and premiums, streamline operational processes, drive down costs, and increase profit margins.


**The project follows the CRISP-DM methodology, which includes the following steps:** <br>
![](/assets/img/wf1.png)
                                                      
As part of my work, I will perform data science and machine learning techniques, including:
1. Integrated Data sets. 
2. Exploratory Data Analysis (EDA). 
3. Feature Engineering. 
4. Feature Selection.
5. Imbalance Date methods. 
6. Machine Learning models. 
7. Evaluation and hyperparameter tuning. <br>
Using **Numpy, Pandas, Matplotlib, Scikit-Learn, and other Python libraries.**

------------------------------------------------------------

### <span style="color:blue">Section 1: Business Understanding</span>.
In this section, we will learn about the business world (the insurance industry) and define the business problem and the objectives of our project.<br>

>**Topics covered:**
>- Introduction to the insurance business model.
>- Terms such as "The Law of Large Numbers", "Expected Value", and "Expected Return".
>- Definition of the business problem and project objectives.


[![](https://img.shields.io/badge/GitHub-Business%20Understanding%20explanation-blue?logo=Github)](https://github.com/Roni-N/Insurance-claim-prediction/blob/gh-pages/Section%201%20Business%20Understanding/(ICP)%200.%20Business%20Understanding..ipynb)

------------------------------------------------------------
### <span style="color:blue">Section 2: Data Understanding</span>.
### Section 2: Data Understanding

#### 2.1 Data integration 
In this section, we will first explore the available data & features from different datasets. 
Next, we will integrate the data into a unified dataset by implementing the business workflow, and finally, we will produce the target variable.<br>
>**Topics covered:**
>- Integrate datasets through business workflow.
>- Comparing Pandas library to SQL basic functions.
>- Produce ground truth (label of target variable). 

[![](https://img.shields.io/badge/GitHub-2.1%20Data%20integration%20code-blue?logo=Github)](https://github.com/Roni-N/Insurance-claim-prediction/blob/gh-pages/Section%202%20Data%20Understanding/2.1%20Data%20integration/(ICP)%201.%20Data%20Grouping%20and%20Aggregation..ipynb)

************************************************************

#### 2.2 Exploratory Data Analysis (EDA)
In this part, we will perform Exploratory Data Analysis (EDA) to perform preliminary investigations, discover patterns, identify anomalies, test hypotheses, and correct assumptions by examining summary statistics and graphs.

[![](https://img.shields.io/badge/GitHub-2.1%20Data%20integration%20code-blue?logo=Github)]()
------------------------------------------------------------
### <span style="color:blue">Section 3: Data preparation </span>.
### Section 3: Data preparation 
#### 3.1 Feature Engineering:
In this section, we will use multiple methods and techniques to convert (process & transform) raw data into features that better represent the underlying problem in a predictive model.<br>
>**Topics covered:**
>- Missing Data Imputation.
>- Categorical Features Encoding.
>- Transformations.
>- Discretisations.
>- Outliers Handling.
>- Features Scaling.
>- Engineering New Features.

************************************************************
#### 3.2 Feature Selection: 
In this part, we will use feature selection methods to reduce the number of input variables (i.e. remove non-informative or redundant features) to those that will provide the greatest level of predictive power for the target variable.<br>
>**Topics covered:**
>- **Filter methods** (variance and statistical test, correlations, univariate selection)
>- **Wrapper methods** (forward / backward selection, exhaustive search)
>- **Embedded methods** (lasso, tree importance)

************************************************************
#### 3.3 Imbalance Data: 
This section will conclude with Imbalanced data handling. Our data set does not have an equally distributed distribution of observations by class. Most standard classifier learning algorithms performed poorly in this issue cloud. It can be resolved by balancing the two classes.<br>
>**Topics covered:**
>- **Undersampling** (Fixed and Cleaning methods).
>- **Oversampling** ( sample extraction and sample generation methods).
>- **Ensemble Method** (data level, cost-sensitive, ensemble algorithms).

************************************************************

------------------------------------------------------------
### <span style="color:blue">Section 4: Machine Learning Algorithms </span>.

### Section 4: Machine Learning Algorithms 
In this section, we will train different machine learning algorithms to create models that could predict future claim insurance premiums.<br>
>**Classification models such as:**
>- Logistic regression.
>- SVM.
>- Naive Bayes classifier. 
>- KNN classifier. 

------------------------------------------------------------
### <span style="color:blue">Section 5: Evaluation and Hyperparameters Tuning  </span>.

### Section 5: Evaluation and Hyperparameters Tuning 
In this section, we will evaluate the model's performance based on relevant performance metrics.
We will also use Hyperparameter Tuning methods (i.e. choosing a set of optimal hyperparameters) to improve model performance.<br>
>**Topics covered:**
>- Cross-validation.
>- Search Algorithms (Grid search, Random search).
>- Bayesian Optimization.

------------------------------------------------------------



------------------------------------------------------------
## <span style="color:blue">Natural Language Processing (NLP) & Text classification - Sentiment analysis and sentiment classification</span>.

In this project, I will conduct a sentiment analysis using statistical methods, in addition to training and evaluating a sentiment classifier for Amazon products reviews using Natural Language Processing. <br>
>**Topics covered:**
>- Cross-validation.
>- Search Algorithms (Grid search, Random search).
>- Bayesian Optimization.

[![](https://img.shields.io/badge/GitHub-Full%20project%20Link-blue?logo=Github)](https://roni-n.github.io/Natural-Language-Processing-NLP-Text-classification/)

